{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                int64\n",
      "workclass         object\n",
      "fnlwgt             int64\n",
      "education         object\n",
      "education-num      int64\n",
      "marital-status    object\n",
      "occupation        object\n",
      "relationship      object\n",
      "race              object\n",
      "sex               object\n",
      "capital-gain       int64\n",
      "capital-loss       int64\n",
      "hours-per-week     int64\n",
      "native-country    object\n",
      "income            object\n",
      "dtype: object\n",
      "   age         workclass  fnlwgt  education  education-num  \\\n",
      "0   39         State-gov   77516  Bachelors             13   \n",
      "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
      "2   38           Private  215646    HS-grad              9   \n",
      "3   53           Private  234721       11th              7   \n",
      "4   28           Private  338409  Bachelors             13   \n",
      "\n",
      "       marital-status         occupation   relationship   race     sex  \\\n",
      "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
      "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
      "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
      "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
      "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
      "\n",
      "   capital-gain  capital-loss  hours-per-week native-country income  \n",
      "0          2174             0              40  United-States  <=50K  \n",
      "1             0             0              13  United-States  <=50K  \n",
      "2             0             0              40  United-States  <=50K  \n",
      "3             0             0              40  United-States  <=50K  \n",
      "4             0             0              40           Cuba  <=50K  \n"
     ]
    }
   ],
   "source": [
    "# Load the training dataset and the test dataset\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "# Check the data types\n",
    "print(train_data.dtypes)\n",
    "# View the first few rows of the dataset\n",
    "print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                int64\n",
      "workclass         object\n",
      "fnlwgt             int64\n",
      "education         object\n",
      "education-num      int64\n",
      "marital-status    object\n",
      "occupation        object\n",
      "relationship      object\n",
      "race              object\n",
      "sex               object\n",
      "capital-gain       int64\n",
      "capital-loss       int64\n",
      "hours-per-week     int64\n",
      "native-country    object\n",
      "income             int64\n",
      "dtype: object\n",
      "age               0\n",
      "workclass         0\n",
      "fnlwgt            0\n",
      "education         0\n",
      "education-num     0\n",
      "marital-status    0\n",
      "occupation        0\n",
      "relationship      0\n",
      "race              0\n",
      "sex               0\n",
      "capital-gain      0\n",
      "capital-loss      0\n",
      "hours-per-week    0\n",
      "native-country    0\n",
      "income            0\n",
      "dtype: int64\n",
      "['White' 'Black' 'Asian-Pac-Islander' 'Amer-Indian-Eskimo' 'Other']\n",
      "['Male' 'Female']\n"
     ]
    }
   ],
   "source": [
    "# Convert the 'income' column to numerical values\n",
    "train_data['income'] = train_data['income'].apply(lambda x: 1 if x == '>50K' else 0)\n",
    "test_data['income'] = test_data['income'].apply(lambda x: 1 if x == '>50K' else 0)\n",
    "# Check the data types\n",
    "print(train_data.dtypes)\n",
    "# Check the number of missing values in each column\n",
    "print(train_data.isnull().sum())\n",
    "# View the unique values in the 'race' column\n",
    "unique_race_values = train_data['race'].unique()\n",
    "print(unique_race_values)\n",
    "# View the unique values in the 'sex' column\n",
    "unique_sex_values = train_data['sex'].unique()\n",
    "print(unique_sex_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical and categorical features\n",
    "numeric_features = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "categorical_features = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
    "\n",
    "# Data preprocessing: Standardize numerical features and one-hot encode categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Create a model pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(max_iter=1000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the training and testing data\n",
    "X_train = train_data.drop(columns=['income'])\n",
    "y_train = train_data['income']\n",
    "X_test = test_data.drop(columns=['income'])\n",
    "y_test = test_data['income']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy: 0.8530\n",
      "Baseline Confusion Matrix:\n",
      "[[11586   849]\n",
      " [ 1544  2302]]\n",
      "Baseline Fairness Metrics for Sex (TPR):\n",
      "Overall Accuracy: 0.8530\n",
      "TPR for sex=Male: 0.6118\n",
      "TPR for sex=Female: 0.5254\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(y_true, y_pred, sensitive_attribute, data):\n",
    "    overall_accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"Overall Accuracy: {overall_accuracy:.4f}\")\n",
    "    \n",
    "    groups = data[sensitive_attribute].unique()\n",
    "    for group in groups:\n",
    "        group_mask = data[sensitive_attribute] == group\n",
    "        # TPR = TP / (TP + FN)\n",
    "        group_tpr = recall_score(y_true[group_mask], y_pred[group_mask])\n",
    "        print(f\"TPR for {sensitive_attribute}={group}: {group_tpr:.4f}\")\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred_baseline = pipeline.predict(X_test)\n",
    "accuracy_baseline = accuracy_score(y_test, y_pred_baseline)\n",
    "print(f\"Baseline Accuracy: {accuracy_baseline:.4f}\")\n",
    "print(\"Baseline Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_baseline))\n",
    "print(\"Baseline Fairness Metrics for Sex (TPR):\")\n",
    "calculate_metrics(y_test, y_pred_baseline, 'sex', test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gender distribution in the original training set:\n",
      "sex\n",
      "Male      21790\n",
      "Female    10771\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# [Data Exploration] Check the number of samples for each gender in the training set\n",
    "print(\"Gender distribution in the original training set:\")\n",
    "print(train_data['sex'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Controlled Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample counts for each gender after intervention:\n",
      "sex\n",
      "Female    19184\n",
      "Male       5929\n",
      "Name: count, dtype: int64\n",
      "Model accuracy after sampling intervention: 0.8094\n",
      "Confusion matrix after sampling intervention:\n",
      "[[10072  2363]\n",
      " [  740  3106]]\n",
      "Fairness metrics for gender (TPR) after sampling intervention:\n",
      "Overall Accuracy: 0.8094\n",
      "TPR for sex=Male: 0.8034\n",
      "TPR for sex=Female: 0.8305\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "# [Intervention Method 1: Controlled Sampling (Improved Version)]\n",
    "# The goal is to adjust the gender ratio in the new training set to 55% males and 45% females (without changing the total number of samples).\n",
    "# %%\n",
    "\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "def p2data(data):\n",
    "    \"\"\"\n",
    "    Intervention at the data level:\n",
    "    Perform random oversampling on the two groups (z=True and z=False) to achieve the target ratio while balancing positive and negative labels within each group.\n",
    "\n",
    "    data: list of (d, z, l), where z is 0/1 (0 for Female, 1 for Male), and l is the label (0/1)\n",
    "    return: list of (d, z, l) (processed data)\n",
    "    \"\"\"\n",
    "    target_z_ratio = 1.8165  # Target ratio: z=False / z=True\n",
    "    # Group by z and label\n",
    "    groups = defaultdict(list)\n",
    "    for d, z, l in data:\n",
    "        groups[(z, l)].append((d, z, l))\n",
    "\n",
    "    # Original counts for each group\n",
    "    group_counts = {k: len(v) for k, v in groups.items()}\n",
    "\n",
    "    # Limit the maximum sampling multiple to prevent excessive oversampling\n",
    "    max_size = max(group_counts.values()) * 5\n",
    "\n",
    "    # Calculate the total number of samples for each z group (regardless of label)\n",
    "    z0_total = group_counts.get((0, True), 0) + group_counts.get((0, False), 0)\n",
    "    z1_total = group_counts.get((1, True), 0) + group_counts.get((1, False), 0)\n",
    "\n",
    "    # Control the overall distribution of z\n",
    "    desired_z0 = min(int(z1_total * target_z_ratio), max_size)\n",
    "    desired_z1 = min(int(z0_total / target_z_ratio), max_size)\n",
    "\n",
    "    def balance_group(pos_count, neg_count, data_pos, data_neg):\n",
    "        # If either label group is empty, simply merge them\n",
    "        if pos_count == 0 or neg_count == 0:\n",
    "            return data_pos + data_neg\n",
    "        # Determine the majority and minority groups\n",
    "        if pos_count < neg_count:\n",
    "            major, minor = data_neg, data_pos\n",
    "        else:\n",
    "            major, minor = data_pos, data_neg\n",
    "        # Oversample the minority class\n",
    "        times = len(major) // len(minor)\n",
    "        remainder = len(major) % len(minor)\n",
    "        sampled = minor * times + random.sample(minor, remainder)\n",
    "        return sampled + major\n",
    "\n",
    "    # Process the z==0 group (corresponding to Female)\n",
    "    balanced_z0 = balance_group(\n",
    "        group_counts.get((0, True), 0),\n",
    "        group_counts.get((0, False), 0),\n",
    "        groups.get((0, True), []),\n",
    "        groups.get((0, False), [])\n",
    "    )\n",
    "    balanced_z0 = random.sample(balanced_z0, min(desired_z0, len(balanced_z0)))\n",
    "\n",
    "    # Process the z==1 group (corresponding to Male)\n",
    "    balanced_z1 = balance_group(\n",
    "        group_counts.get((1, True), 0),\n",
    "        group_counts.get((1, False), 0),\n",
    "        groups.get((1, True), []),\n",
    "        groups.get((1, False), [])\n",
    "    )\n",
    "    balanced_z1 = random.sample(balanced_z1, min(desired_z1, len(balanced_z1)))\n",
    "\n",
    "    balanced_data = balanced_z0 + balanced_z1\n",
    "    random.shuffle(balanced_data)\n",
    "    return balanced_data\n",
    "\n",
    "# Convert DataFrame to a list of (d, z, l); here d is the entire row data\n",
    "def df2list(df):\n",
    "    data_list = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Convention: Female -> z=0, Male -> z=1\n",
    "        z = 0 if row['sex'] == 'Female' else 1\n",
    "        l = row['income']\n",
    "        # Pass the entire row directly (use pd.DataFrame(list_of_d) when converting back to DataFrame)\n",
    "        data_list.append((row.to_dict(), z, l))\n",
    "    return data_list\n",
    "\n",
    "def list2df(data_list):\n",
    "    # Extract the d dictionary and construct a DataFrame\n",
    "    return pd.DataFrame([d for d, z, l in data_list])\n",
    "\n",
    "# Apply sampling intervention to the training data\n",
    "data_list = df2list(train_data)\n",
    "balanced_data_list = p2data(data_list)\n",
    "train_data_balanced = list2df(balanced_data_list)\n",
    "\n",
    "print(\"Sample counts for each gender after intervention:\")\n",
    "print(train_data_balanced['sex'].value_counts())\n",
    "\n",
    "# Train the model using the balanced data\n",
    "X_train_bal = train_data_balanced.drop(columns=['income'])\n",
    "y_train_bal = train_data_balanced['income']\n",
    "\n",
    "pipeline.fit(X_train_bal, y_train_bal)\n",
    "y_pred_bal = pipeline.predict(X_test)\n",
    "accuracy_bal = accuracy_score(y_test, y_pred_bal)\n",
    "print(f\"Model accuracy after sampling intervention: {accuracy_bal:.4f}\")\n",
    "print(\"Confusion matrix after sampling intervention:\")\n",
    "print(confusion_matrix(y_test, y_pred_bal))\n",
    "print(\"Fairness metrics for gender (TPR) after sampling intervention:\")\n",
    "calculate_metrics(y_test, y_pred_bal, 'sex', test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Weight Adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy with dynamic weight adjustment: 0.8485\n",
      "Confusion matrix with dynamic weight adjustment:\n",
      "[[11455   980]\n",
      " [ 1486  2360]]\n",
      "Fairness metrics for gender (TPR) with dynamic weight adjustment:\n",
      "Overall Accuracy: 0.8485\n",
      "TPR for sex=Male: 0.6139\n",
      "TPR for sex=Female: 0.6119\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "# [Intervention Method 2: Model Weight Adjustment]\n",
    "# Use the `class_weight` parameter of LogisticRegression to automatically adjust the weights of different classes (income) to mitigate the impact of class imbalance on model training, thereby indirectly improving the performance of different sensitive groups (e.g., gender).\n",
    "# Note: Here, `class_weight` adjusts the weights of the income labels, which is a common in-processing intervention method.\n",
    "\n",
    "# %%\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Calculate dynamic weights: Add an auxiliary column 'z' to X_train, mapping 'Female' to 0 and 'Male' to 1\n",
    "X_train_weight = X_train.copy()\n",
    "X_train_weight['z'] = X_train_weight['sex'].apply(lambda s: 0 if s == 'Female' else 1)\n",
    "\n",
    "# Count the number of positive examples in each group (z==0 for Female, z==1 for Male)\n",
    "z0_pos = sum((X_train_weight['z'] == 0) & (y_train == 1))\n",
    "z1_pos = sum((X_train_weight['z'] == 1) & (y_train == 1))\n",
    "\n",
    "# Calculate dynamic weights based on the sensitive attribute and label of each sample\n",
    "weights = []\n",
    "for i, row in X_train_weight.iterrows():\n",
    "    z_val = row['z']\n",
    "    l_val = y_train.iloc[i]\n",
    "    # Dynamic weight calculation logic: Increase the weight of positive examples for the disadvantaged group (Female, z==0), and slightly suppress the weight for the advantaged group (Male, z==1)\n",
    "    if z_val == 0:  # Disadvantaged group (Female)\n",
    "        weight = 5.2 if l_val == 1 else 3.2\n",
    "    else:           # Advantaged group (Male)\n",
    "        weight = 1 if l_val == 1 else 1\n",
    "    # Automatically adjust based on the total number of positive examples (to prevent excessive weight differences)\n",
    "    if z_val == 0:\n",
    "        weight *= (z0_pos + z1_pos) / (z0_pos + 1e-5)\n",
    "    else:\n",
    "        weight *= (z0_pos + z1_pos) / (z1_pos + 1e-5)\n",
    "    weights.append(weight)\n",
    "\n",
    "# Construct the model pipeline using the preprocessor and LogisticRegression classifier\n",
    "model_dynamic = Pipeline([\n",
    "    ('preprocessor', preprocessor),  # Preprocessor: Standardize numerical features and one-hot encode categorical features\n",
    "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "# Train the model with the dynamically calculated sample_weight\n",
    "model_dynamic.fit(X_train, y_train, classifier__sample_weight=weights)\n",
    "y_pred_dynamic = model_dynamic.predict(X_test)\n",
    "\n",
    "accuracy_dynamic = accuracy_score(y_test, y_pred_dynamic)\n",
    "print(f\"Model accuracy with dynamic weight adjustment: {accuracy_dynamic:.4f}\")\n",
    "print(\"Confusion matrix with dynamic weight adjustment:\")\n",
    "print(confusion_matrix(y_test, y_pred_dynamic))\n",
    "print(\"Fairness metrics for gender (TPR) with dynamic weight adjustment:\")\n",
    "calculate_metrics(y_test, y_pred_dynamic, 'sex', test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reweighting with Smoothing(Based on the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Reweighting: Average smoothed weights for each gender group:\n",
      "sex\n",
      "Female    1.0\n",
      "Male      1.0\n",
      "dtype: float64\n",
      "Adjusted Reweighting Model Accuracy: 0.8509\n",
      "Adjusted Reweighting Confusion Matrix:\n",
      "[[11597   838]\n",
      " [ 1590  2256]]\n",
      "Adjusted Reweighting Fairness Metrics for Sex (TPR):\n",
      "Overall Accuracy: 0.8509\n",
      "TPR for sex=Male: 0.5866\n",
      "TPR for sex=Female: 0.5864\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "# [Intervention Method 3: Reweighting with Smoothing]\n",
    "# Based on the formula proposed by Kamiran and Calders (2011):\n",
    "# [Improved Reweighting Method: Introducing a Smoothing Parameter]\n",
    "# Original formula: weight = (n_{sex} * n_{income}) / (n_total * n_{sex,income})\n",
    "# To avoid over-adjustment (which may significantly reduce the performance for males while overly boosting it for females), \n",
    "# we apply a smoothing technique to the original weights using interpolation: \n",
    "# new_weight = (1 - lambda) * 1 + lambda * original_weight, \n",
    "# where lambda is the smoothing parameter (0 < lambda < 1). A smaller lambda makes the new weights closer to 1 (i.e., no reweighting effect), \n",
    "# while a larger lambda makes the new weights closer to the original values. You can adjust lambda_param to balance overall performance and fairness.\n",
    "\n",
    "lambda_param = 0.3  # Adjustable parameter: e.g., 0.5 means the new weight is the average of the original weight and 1\n",
    "n = len(train_data)\n",
    "group_counts = train_data.groupby('sex').size().to_dict()       # n_{sex}\n",
    "label_counts = train_data.groupby('income').size().to_dict()      # n_{income}\n",
    "joint_counts = train_data.groupby(['sex', 'income']).size().to_dict()  # n_{sex,income}\n",
    "\n",
    "def compute_weight(sex, income):\n",
    "    n_a = group_counts[sex]\n",
    "    n_y = label_counts[income]\n",
    "    n_a_y = joint_counts[(sex, income)]\n",
    "    return (n_a * n_y) / (n * n_a_y)\n",
    "# Calculate original weights\n",
    "sample_weights = train_data.apply(lambda row: compute_weight(row['sex'], row['income']), axis=1)\n",
    "\n",
    "# Smoothing: Adjusted weights\n",
    "adjusted_weights = (1 - lambda_param) + lambda_param * sample_weights\n",
    "\n",
    "print(\"Adjusted Reweighting: Average smoothed weights for each gender group:\")\n",
    "print(train_data.groupby('sex').apply(\n",
    "    lambda df: np.mean((1 - lambda_param) + lambda_param * df.apply(lambda row: compute_weight(row['sex'], row['income']), axis=1))\n",
    "))\n",
    "\n",
    "# Train the model using the adjusted weights\n",
    "pipeline.fit(X_train, y_train, classifier__sample_weight=adjusted_weights)\n",
    "y_pred_rew_adjusted = pipeline.predict(X_test)\n",
    "accuracy_rew_adjusted = accuracy_score(y_test, y_pred_rew_adjusted)\n",
    "print(f\"Adjusted Reweighting Model Accuracy: {accuracy_rew_adjusted:.4f}\")\n",
    "print(\"Adjusted Reweighting Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_rew_adjusted))\n",
    "print(\"Adjusted Reweighting Fairness Metrics for Sex (TPR):\")\n",
    "calculate_metrics(y_test, y_pred_rew_adjusted, 'sex', test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Massaging(Based on the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Massaging: Original number of labels to flip (delta): 2741, Adjusted number of flips: 186\n",
      "Adjusted Massaging: Comparison of positive example counts before and after flipping:\n",
      "Female: Original positive count = 1179   After change = 1365\n",
      "Male: Original positive count = 6662   After change = 6476\n",
      "Adjusted Massaging Model Accuracy: 0.8506\n",
      "Adjusted Massaging Confusion Matrix:\n",
      "[[11495   940]\n",
      " [ 1493  2353]]\n",
      "Adjusted Massaging Fairness Metrics for Sex (TPR):\n",
      "Overall Accuracy: 0.8506\n",
      "TPR for sex=Male: 0.6121\n",
      "TPR for sex=Female: 0.6102\n"
     ]
    }
   ],
   "source": [
    "# %% \n",
    "# [Intervention Method 4: Massaging]\n",
    "# Based on the massaging technique by Kamiran and Calders (2011): Flip the labels of some samples near the decision boundary to reduce the gap in positive example ratios between different gender groups while maintaining model performance as much as possible.\n",
    "# 1. Train a logistic regression model on the original data to calculate the positive class probabilities (scores) for each sample.\n",
    "# 2. For the unprivileged group (Female), flip the labels of negative examples with higher scores; for the privileged group (Male), flip the labels of positive examples with lower scores.\n",
    "# %%\n",
    "\n",
    "# [Improved Massaging Method: Introducing the Flip Ratio Parameter alpha]\n",
    "# To avoid too many positive examples in the unprivileged group (Female), we introduce a parameter alpha (0 < alpha <= 1) to control the actual number of samples to be flipped after calculating the original number of flips (delta).\n",
    "# A smaller alpha means gentler flipping; for example, alpha = 0.5 means flipping only half of the originally required number.\n",
    "\n",
    "alpha = 0.068  # Flip ratio parameter, adjustable based on experimental results\n",
    "\n",
    "# Calculate the original number of flips (delta) as before\n",
    "X_train_trans = preprocessor.fit_transform(X_train)\n",
    "lr_massaging = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_massaging.fit(X_train_trans, y_train)\n",
    "scores = lr_massaging.predict_proba(X_train_trans)[:, 1]\n",
    "\n",
    "# raw data、score and label\n",
    "train_massaging = X_train.copy()\n",
    "train_massaging['income'] = y_train.values\n",
    "train_massaging['score'] = scores\n",
    "privileged = 'Male'\n",
    "unprivileged = 'Female'\n",
    "n_priv_pos = train_massaging[train_massaging['sex'] == privileged]['income'].sum()\n",
    "n_unpriv_pos = train_massaging[train_massaging['sex'] == unprivileged]['income'].sum()\n",
    "delta = int(abs(n_priv_pos - n_unpriv_pos) / 2)\n",
    "n_priv_pos = train_massaging[train_massaging['sex'] == privileged]['income'].sum()\n",
    "n_unpriv_pos = train_massaging[train_massaging['sex'] == unprivileged]['income'].sum()\n",
    "delta = int(abs(n_priv_pos - n_unpriv_pos) / 2)\n",
    "\n",
    "\n",
    "# Actual number of flips\n",
    "new_delta = int(alpha * delta)\n",
    "print(f\"Massaging: Original number of labels to flip (delta): {delta}, Adjusted number of flips: {new_delta}\")\n",
    "\n",
    "# For Female (unprivileged) negative examples: Select the top new_delta candidates with the highest scores for flipping\n",
    "candidates_unpriv = train_massaging[(train_massaging['sex'] == unprivileged) & (train_massaging['income'] == 0)]\n",
    "candidates_unpriv = candidates_unpriv.sort_values(by='score', ascending=False)\n",
    "indices_to_flip_unpriv = candidates_unpriv.index[:new_delta]\n",
    "\n",
    "# For Male (privileged) positive examples: Select the bottom new_delta candidates with the lowest scores for flipping\n",
    "candidates_priv = train_massaging[(train_massaging['sex'] == privileged) & (train_massaging['income'] == 1)]\n",
    "candidates_priv = candidates_priv.sort_values(by='score', ascending=True)\n",
    "indices_to_flip_priv = candidates_priv.index[:new_delta]\n",
    "\n",
    "# Flip labels: Change negative examples in unprivileged group to positive, and positive examples in privileged group to negative\n",
    "y_train_massaged_adj = y_train.copy()\n",
    "y_train_massaged_adj.loc[indices_to_flip_unpriv] = 1\n",
    "y_train_massaged_adj.loc[indices_to_flip_priv] = 0\n",
    "\n",
    "print(\"Adjusted Massaging: Comparison of positive example counts before and after flipping:\")\n",
    "print(\"Female: Original positive count =\", train_massaging[train_massaging['sex'] == unprivileged]['income'].sum(),\n",
    "      \"  After change =\", y_train_massaged_adj[train_massaging['sex'] == unprivileged].sum())\n",
    "print(\"Male: Original positive count =\", train_massaging[train_massaging['sex'] == privileged]['income'].sum(),\n",
    "      \"  After change =\", y_train_massaged_adj[train_massaging['sex'] == privileged].sum())\n",
    "\n",
    "# Train the model with the adjusted labels\n",
    "pipeline.fit(X_train, y_train_massaged_adj)\n",
    "y_pred_massaged_adj = pipeline.predict(X_test)\n",
    "accuracy_massaged_adj = accuracy_score(y_test, y_pred_massaged_adj)\n",
    "print(f\"Adjusted Massaging Model Accuracy: {accuracy_massaged_adj:.4f}\")\n",
    "print(\"Adjusted Massaging Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_massaged_adj))\n",
    "print(\"Adjusted Massaging Fairness Metrics for Sex (TPR):\")\n",
    "calculate_metrics(y_test, y_pred_massaged_adj, 'sex', test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy and TPR comparison for males and females across all intervention methods:\n",
      "                      Method  Accuracy  Male TPR  Female TPR\n",
      "0                   Baseline  0.853019  0.611794    0.525424\n",
      "1      Controlled Resampling  0.809410  0.803440    0.830508\n",
      "2  Dynamic Weight Adjustment  0.848535  0.613943    0.611864\n",
      "3       Adjusted Reweighting  0.850869  0.586609    0.586441\n",
      "4         Adjusted Massaging  0.850562  0.612101    0.610169\n"
     ]
    }
   ],
   "source": [
    "def get_group_tpr(y_true, y_pred, group_value, sensitive_attribute='sex'):\n",
    "    mask = test_data[sensitive_attribute] == group_value\n",
    "    return recall_score(y_true[mask], y_pred[mask])\n",
    "\n",
    "baseline_male_tpr = get_group_tpr(y_test, y_pred_baseline, 'Male')\n",
    "baseline_female_tpr = get_group_tpr(y_test, y_pred_baseline, 'Female')\n",
    "\n",
    "res_male_tpr = get_group_tpr(y_test, y_pred_bal, 'Male')\n",
    "res_female_tpr = get_group_tpr(y_test, y_pred_bal, 'Female')\n",
    "\n",
    "dynamic_male_tpr = get_group_tpr(y_test, y_pred_dynamic, 'Male')\n",
    "dynamic_female_tpr = get_group_tpr(y_test, y_pred_dynamic, 'Female')\n",
    "\n",
    "rew_male_tpr = get_group_tpr(y_test, y_pred_rew_adjusted, 'Male')\n",
    "rew_female_tpr = get_group_tpr(y_test, y_pred_rew_adjusted, 'Female')\n",
    "\n",
    "massaged_male_tpr = get_group_tpr(y_test, y_pred_massaged_adj, 'Male')\n",
    "massaged_female_tpr = get_group_tpr(y_test, y_pred_massaged_adj, 'Female')\n",
    "\n",
    "results = {\n",
    "    'Method': ['Baseline', 'Controlled Resampling', 'Dynamic Weight Adjustment', 'Adjusted Reweighting', 'Adjusted Massaging'],\n",
    "    'Accuracy': [accuracy_baseline, accuracy_bal, accuracy_dynamic, accuracy_rew_adjusted, accuracy_massaged_adj],\n",
    "    'Male TPR': [baseline_male_tpr, res_male_tpr, dynamic_male_tpr, rew_male_tpr, massaged_male_tpr],\n",
    "    'Female TPR': [baseline_female_tpr, res_female_tpr, dynamic_female_tpr, rew_female_tpr, massaged_female_tpr]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Overall accuracy and TPR comparison for males and females across all intervention methods:\")\n",
    "print(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
